##
# Utility functions for Loading Data
##

from numpy import *
import numpy as np
from collections import defaultdict

# given a dataset containing of variable length sequences
# generates batches, each contaning "batch_size" number of samples              
# Each sample is generated by using 
# input_sequence -> 0 to seqlen-1 vertices as input
# output_seuqnce -> 1 to seqLen vertices as output
# additionally this also generates the length of each sequence = 
# length of input_sequence in batch = length of actual sequence-1
def get_pathsearch_dataset_batch_varlen(dataset, batch_size, max_steps):
	fn = 'data/yago2data/{}.ttl'
	data_array = []   
	label_array = []
	dest_array = []
	seqL_array = []
	len_max = 2	
	for line in open(fn.format(dataset)):
		count = 0        
		word_array = []
		for word in line.strip().split("\t"):
			word_array.append(int(word))
			count += 1
			if count >= max_steps:
				break			 
		dest_array.append(word_array[-1])
		data_len = len(word_array)
		seqL_array.append(data_len-1)  
		if data_len < max_steps:
			for ii in xrange(max_steps-data_len):
				word_array.append(int(0.0))
		data_array.append(word_array[:-1])
		label_array.append(word_array[1:])
	data_len = len(data_array)
	epoch_size = data_len//batch_size
	data_len = len(data_array)    
	epoch_size = data_len//batch_size
	for i in range(epoch_size):
		x = data_array[i*batch_size:(i+1)*batch_size]    
		y = label_array[i*batch_size:(i+1)*batch_size]
		dest_id = dest_array[i*batch_size:(i+1)*batch_size]
		seqL = seqL_array[i*batch_size:(i+1)*batch_size]
		yield (x,y, dest_id, seqL)
		
def load_vertex(vertexfile):
    with open(vertexfile) as fd:
        nodes_info = [line.strip().split("\t") for line in fd]
    num_nodes = len(nodes_info)
    nodes_info = np.reshape(nodes_info, [num_nodes,2])
    num_to_word = nodes_info[:,1]
    word_to_num = nodes_info[:,0]
    return word_to_num, num_to_word
